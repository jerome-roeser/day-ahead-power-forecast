{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from colorama import Fore, Style\n",
    "import datetime as dt\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from google.cloud import bigquery, storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, MinMaxScaler, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, make_union\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
    "\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Collected data**\n",
    "\n",
    "#### 📌 Tempelhofer Feld, Berlin, Germany"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe src=\"https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d26842.24808091901!2d13.400488957085193!3d52.477045271245125!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x47a84fe8f7d899eb%3A0x88898e99acbb718b!2sTempelhofer%20Feld!5e0!3m2!1sen!2sde!4v1736167638260!5m2!1sen!2sde\" width=\"1200\" height=\"450\" style=\"border:0;\" allowfullscreen=\"\" loading=\"lazy\" referrerpolicy=\"no-referrer-when-downgrade\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ☀️ 1. Historical photovoltaic production data\n",
    "* 42 years (1980 - 2022)\n",
    "* hourly resolution\n",
    "* synthetic data derived from measured irradiance data\n",
    "* ~ 400 000 data points\n",
    "* source: [Renewables.ninja](https://www.renewables.ninja/about \"Renewables.ninja\")\n",
    "\n",
    "### 🌡️ 2. Historical weather forecast data\n",
    "* 6 1/2 years (Oct 2017 - Mar 2024)\n",
    "* 4 model recalculation cycles available for each day (updated at: 00:00 UTC, 06:00 UTC, 12:00 UTC, 18:00 UTC)\n",
    "* Each 16 days forecast in hourly resolution\n",
    "* ~ 3.3 M rows - 24 Features\n",
    "* source: [OpenWeather](https://openweathermap.org/api/history-forecast-bulk \"History-forecast-bulk\") (40 €)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_data = pd.read_csv('../data/1980-2022_pv.csv', memory_map=True)\n",
    "forecast_data = pd.read_csv('../data/openweather_history_bulk_forecast_tempelhof.csv', memory_map=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Check the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Photovolatic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_data.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 check for time gaps in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of rows with unique values in our data\n",
    "local_time_series = pd.to_datetime(pv_data['local_time'], utc= True)\n",
    "\n",
    "unique_timestamps_pv_dataset = local_time_series.nunique()\n",
    "unique_timestamps_pv_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of rows with unique values of a synthetic data with the same range\n",
    "min_date = local_time_series.min()\n",
    "max_date = local_time_series.max()\n",
    "unique_timestamps_pv_dataset_theoretical = pd.date_range(start= min_date, end= max_date, freq=dt.timedelta(hours=1.0)).nunique()\n",
    "unique_timestamps_pv_dataset_theoretical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert unique_timestamps_pv_dataset == unique_timestamps_pv_dataset_theoretical\n",
    "    print('all good! there are no gaps in your dataset...')\n",
    "except AssertionError:\n",
    "    print('there are gaps in your dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Photovolatic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_encoding(X: pd.DataFrame, column_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert local time into cyclic features to feed significant signal\n",
    "    in ML / DL algorithm\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): datafrane to transform\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with 4 addtional features per time column\n",
    "    \"\"\"\n",
    "    X = X.copy()\n",
    "\n",
    "    local_time = pd.to_datetime(X.pop(column_name), utc= True)\n",
    "    timestamp_s = local_time.map(pd.Timestamp.timestamp)\n",
    "\n",
    "\n",
    "    day = 24*60*60\n",
    "    year = (365.2425)*day\n",
    "    X['day_sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
    "    X['day_cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
    "    X['year_sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
    "    X['year_cos'] = np.cos(timestamp_s * (2 * np.pi / year))\n",
    "\n",
    "    return X.iloc[:,-4:]\n",
    "\n",
    "\n",
    "time_encoder = FunctionTransformer(time_encoding, kw_args={'column_name': 'local_time'})\n",
    "\n",
    "pv_transformer = ColumnTransformer(\n",
    "    [\n",
    "        ('Time Encoder', time_encoder, ['local_time']),\n",
    "        ('Passthrough', 'passthrough', ['local_time', 'electricity' ]),\n",
    "    ],\n",
    "    remainder='drop'\n",
    ").set_output(transform='pandas')\n",
    "\n",
    "preprocess_pv = Pipeline([('Time Features', pv_transformer)])\n",
    "pv_processed = preprocess_pv.fit_transform(pv_data)\n",
    "\n",
    "pv_processed.columns = [column.split('__')[1] for column in pv_processed.columns]\n",
    "pv_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols_pv = pv_processed.select_dtypes(include=np.number).columns\n",
    "melted_pv_df = pd.melt(pv_processed[num_cols_pv])\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "sns.violinplot(data=melted_pv_df.sample(10_000), x='variable', y='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_indices = {name:i for i, name in enumerate(pv_processed.columns)}\n",
    "column_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Weather forecast data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(df: pd.DataFrame, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reduces size of dataframe by downcasting numerical columns\n",
    "    \"\"\"\n",
    "    input_size = df.memory_usage(index=True).sum() / 1024\n",
    "    print(\"dataframe size: \", round(input_size,2), 'kB')\n",
    "\n",
    "    in_size = df.memory_usage(index=True).sum()\n",
    "    for type in [\"float\", \"integer\"]:\n",
    "        l_cols = list(df.select_dtypes(include=type))\n",
    "        for col in l_cols:\n",
    "            df[col] = pd.to_numeric(df[col], downcast=type)\n",
    "    out_size = df.memory_usage(index=True).sum()\n",
    "    ratio = (1 - round(out_size / in_size, 2)) * 100\n",
    "\n",
    "    print(\"optimized size by {} %\".format(round(ratio,2)))\n",
    "    print(\"new dataframe size: \", round(out_size / 1024,2), \" kB\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def clean_forecast_data(forecast_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Initial has 3.3 M entries (everyday: 4 forecasts of 16 days ahead)\n",
    "    Cleaning it to: - 1 forecast perday (at 12:00)\n",
    "                    - 48 hours a day\n",
    "                    - right now hardcoded to match last forecast day with\n",
    "                     last day of PV data\n",
    "    \"\"\"\n",
    "    df = compress(forecast_df)\n",
    "    df = df.drop(columns=['lat', 'lon',\n",
    "                          'forecast dt iso',\n",
    "                          'slice dt iso'])\n",
    "\n",
    "    df.rename(columns={'forecast dt unixtime':'utc_time',\n",
    "                        'slice dt unixtime':'prediction_utc_time'},\n",
    "                        inplace=True)\n",
    "\n",
    "    # df['utc_time'] = df['utc_time'].str.replace('+0000 UTC', '')\n",
    "    # df['prediction_utc_time'] = df['prediction_utc_time'].str.replace('+0000 UTC', '')\n",
    "\n",
    "    df['utc_time'] = pd.to_datetime(df['utc_time'], unit= 's', utc= True)\n",
    "    df['prediction_utc_time'] = pd.to_datetime(df['prediction_utc_time'], unit= 's', utc= True)\n",
    "\n",
    "    # # get only 1 forecast per day\n",
    "    df = df[df['utc_time'].dt.hour == 12]\n",
    "\n",
    "    unique_dates = df['utc_time'].unique()\n",
    "\n",
    "    # reduce to 24h of weather forecast (from 00:00 to 23:00 each day)\n",
    "    df_revised = []\n",
    "    for date in unique_dates:\n",
    "        data = df[(df['utc_time'] == date)].iloc[12:36]\n",
    "        df_revised.append(data)\n",
    "\n",
    "    df_revised_ordered = pd.concat(df_revised, ignore_index=True)\n",
    "\n",
    "    # # hard code the end date to match wiht PV data\n",
    "    # processed_df = df_revised_ordered[df_revised_ordered['prediction_utc_time'] <= '2022-12-31 23:00:00']\n",
    "\n",
    "    return df_revised_ordered\n",
    "\n",
    "forecast_clean = clean_forecast_data(forecast_data)\n",
    "forecast_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_encoding(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert local time into cyclic features to feed significant signal\n",
    "    in ML / DL algorithm\n",
    "\n",
    "    Args:\n",
    "    X: datafrane to transform\n",
    "\n",
    "    Output:\n",
    "    DataFrame with 4 addtional features per time column\n",
    "    \"\"\"\n",
    "    X = X.copy()\n",
    "\n",
    "    date_time = pd.to_datetime(X.pop('prediction_utc_time'), utc= True)\n",
    "    timestamp_s = date_time.map(pd.Timestamp.timestamp)\n",
    "\n",
    "    day = 24*60*60\n",
    "    year = (365.2425)*day\n",
    "    X['forecast_day_sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
    "    X['forecast_day_cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
    "    X['forecast_year_sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
    "    X['forecast_year_cos'] = np.cos(timestamp_s * (2 * np.pi / year))\n",
    "\n",
    "    return X.iloc[:,-4:]\n",
    "\n",
    "def wind_encoding(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert local time into cyclic features to feed significant signal\n",
    "    in ML / DL algorithm\n",
    "\n",
    "    Args:\n",
    "    X: datafrane to transform\n",
    "\n",
    "    Output:\n",
    "    DataFrame with 4 addtional features per time column\n",
    "    \"\"\"\n",
    "    X = X.copy()\n",
    "\n",
    "    # Process wind fratures\n",
    "    wind_speed = X.pop('wind_speed')\n",
    "\n",
    "    # Convert to radians.\n",
    "    wind_rad = X.pop('wind_deg')*np.pi / 180\n",
    "\n",
    "    # Calculate the wind x and y components\n",
    "    X['Wx'] = wind_speed*np.cos(wind_rad)\n",
    "    X['Wy'] = wind_speed*np.sin(wind_rad)\n",
    "\n",
    "    # Standardize the components\n",
    "    X['Wx'] = (X['Wx'] - X['Wx'].mean())/X['Wx'].std()\n",
    "    X['Wy'] = (X['Wy'] - X['Wy'].mean())/X['Wy'].std()\n",
    "\n",
    "    return X.iloc[:,-2:]\n",
    "\n",
    "std_features = ['temperature', 'dew_point', 'pressure', 'ground_pressure',\n",
    "                'humidity',]\n",
    "minmax_features = ['clouds', 'rain', 'snow', 'ice', 'fr_rain', 'convective',\n",
    "                   'snow_depth', 'accumulated', 'hours', 'rate', 'probability']\n",
    "\n",
    "time_encoder = FunctionTransformer(time_encoding)\n",
    "wind_encoder = FunctionTransformer(wind_encoding)\n",
    "\n",
    "forecast_transformer = ColumnTransformer(\n",
    "    [\n",
    "        ('Time Encoder', time_encoder, ['prediction_utc_time']),\n",
    "        ('Wind Encoder', wind_encoder, ['wind_speed', 'wind_deg']),\n",
    "        ('Std', StandardScaler(), std_features),\n",
    "        ('MinMax', MinMaxScaler(), minmax_features),\n",
    "        # ('Drop', 'drop', ['local_time', 'utc_time', 'prediction_utc_time']),\n",
    "        ('Passthrough', 'passthrough', ['utc_time', 'prediction_utc_time']),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ").set_output(transform='pandas')\n",
    "\n",
    "\n",
    "forecast_processed = forecast_transformer.fit_transform(forecast_clean)\n",
    "\n",
    "forecast_processed.columns = [column.split('__')[1] for column in forecast_processed.columns]\n",
    "display(forecast_processed)\n",
    "forecast_transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stick electricity data to the forecast data\n",
    "pv_data['local_time'] = pd.to_datetime(pv_data['local_time'], utc= True)\n",
    "merged_forecast_pv = pd.merge(pv_data[['local_time', 'electricity']], forecast_processed, left_on='local_time', right_on= 'prediction_utc_time', how='inner')\n",
    "forecast_processed = merged_forecast_pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "num_cols_forecast = forecast_processed.select_dtypes(include=np.number).columns\n",
    "\n",
    "plt.subplot(311)\n",
    "melted_forecast_df = pd.melt(forecast_processed[num_cols_forecast].iloc[:, :8])\n",
    "sns.violinplot(data= melted_forecast_df.sample(frac=0.25), x='variable', y='value')\n",
    "\n",
    "plt.subplot(312)\n",
    "melted_forecast_df = pd.melt(forecast_processed[num_cols_forecast].iloc[:, 8:16])\n",
    "sns.violinplot(data= melted_forecast_df.sample(frac=0.25), x='variable', y='value')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(313)\n",
    "melted_forecast_df = pd.melt(forecast_processed[num_cols_forecast].iloc[:, 16:])\n",
    "sns.violinplot(data= melted_forecast_df.sample(frac=0.25), x='variable', y='value')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_processed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Sequencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Photovoltaic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is to train the model on a high number of sequences. Time series data have that in particular that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Parameters ################\n",
    "#################################\n",
    "\n",
    "INPUT_WIDTH = 48\n",
    "LABEL_WIDTH = 24\n",
    "SHIFT = 36\n",
    "\n",
    "class SequenceGenerator:\n",
    "    \"\"\" \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_width: int,\n",
    "        label_width: int,\n",
    "        shift: int,\n",
    "        number_sequences: int,\n",
    "        train_df: pd.DataFrame,\n",
    "        val_df: pd.DataFrame = None,\n",
    "        test_df: pd.DataFrame = None,\n",
    "        label_columns: List[str] = None,\n",
    "    ):\n",
    "        # Store the raw data.\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "\n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {\n",
    "                name: i for i, name in enumerate(label_columns)\n",
    "            }\n",
    "        self.column_indices = {name: i for i, name in enumerate(train_df.columns)}\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.number_of_sequences = number_sequences\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, self.total_window_size)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"\\n\".join(\n",
    "            [\n",
    "                f\"Number of sequences: {self.number_of_sequences}\",\n",
    "                f\"Total window size: {self.total_window_size}\",\n",
    "                f\"Input indices: {self.input_indices}\",\n",
    "                f\"Label indices: {self.label_indices}\",\n",
    "                f\"Label column name(s): {self.label_columns}\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def split_window(self, features):\n",
    "        inputs = features[self.input_slice, :]\n",
    "        labels = features[self.labels_slice, :]\n",
    "        if self.label_columns is not None:\n",
    "            labels = np.stack(\n",
    "                [labels[:, self.column_indices[name]] for name in self.label_columns],\n",
    "                axis=-1,\n",
    "            )\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def make_dataset(self, data: pd.DataFrame):\n",
    "        \"Create a dataset of x sequences of features and labels\"\n",
    "        data = np.array(data)\n",
    "        last_full_sequence_start = len(data) - self.total_window_size\n",
    "        inputs, labels = [], []\n",
    "\n",
    "        for n in tqdm(range(self.number_of_sequences)):\n",
    "            random_start = np.random.randint(0, last_full_sequence_start)\n",
    "            input, label = self.split_window(data[random_start:])\n",
    "\n",
    "            inputs.append(torch.tensor(input, dtype=torch.float32))\n",
    "            labels.append(torch.tensor(label, dtype=torch.float32))\n",
    "\n",
    "        inputs = torch.cat(inputs).view(self.number_of_sequences, self.input_width, -1)\n",
    "        labels = torch.cat(labels).view(self.number_of_sequences, self.label_width, -1)\n",
    "        return TensorDataset(inputs, labels)\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.make_dataset(self.train_df)\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        return self.make_dataset(self.val_df)\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        return self.make_dataset(self.test_df)\n",
    "\n",
    "    @property\n",
    "    def example(self):\n",
    "        \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "        result = getattr(self, \"_example\", None)\n",
    "        if result is None:\n",
    "            # No example batch was found, so get one from the `.train` dataset\n",
    "            result = next(iter(self.train))\n",
    "            # And cache it for next time\n",
    "            self._example = result\n",
    "        return result\n",
    "\n",
    "\n",
    "seq_pv = SequenceGenerator(input_width= INPUT_WIDTH,\n",
    "                              label_width= LABEL_WIDTH,\n",
    "                              shift= SHIFT,\n",
    "                              number_sequences= 10_000,\n",
    "                              train_df= pv_processed,\n",
    "                              label_columns= ['electricity'])\n",
    "\n",
    "seq_pv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Forecast Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceForecastDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        number_days_forecast: int = 1,\n",
    "        label_columns: List[str] = None,\n",
    "    ):\n",
    "        self.df = df.copy().astype(\"float32\")\n",
    "        self.forecast_hours = number_days_forecast * 24\n",
    "        self.number_of_sequences = len(self.df) // self.forecast_hours\n",
    "\n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {\n",
    "                name: i for i, name in enumerate(label_columns)\n",
    "            }\n",
    "        self.column_indices = {name: i for i, name in enumerate(self.df.columns)}\n",
    "        self.feature_columns = [\n",
    "            col for col in self.df.columns if col not in self.label_columns\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.number_of_sequences\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inputs = self.df[self.feature_columns].values\n",
    "        if self.label_columns is not None:\n",
    "            labels = self.df[[name for name in self.label_columns]].values\n",
    "        else:\n",
    "            labels = self.df[\"electricity\"].values\n",
    "        return inputs.reshape(self.number_of_sequences, self.forecast_hours, -1)[index], \\\n",
    "                labels.reshape(self.number_of_sequences, self.forecast_hours, -1)[index]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"\\n\".join(\n",
    "            [\n",
    "                f\"Number of sequences: {self.number_of_sequences}\",\n",
    "                f\"Label column name(s): {self.label_columns}\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def example(self):\n",
    "        \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "        result = getattr(self, \"_example\", None)\n",
    "        if result is None:\n",
    "            # No example batch was found, so get a random one from the dataset\n",
    "            random_idx = np.random.randint(0, self.number_of_sequences)\n",
    "            result = self[random_idx]\n",
    "            # And cache it for next time\n",
    "            self._example = result\n",
    "        return result\n",
    "\n",
    "seq_forecast = SequenceForecastDataset(forecast_processed.select_dtypes(include=np.number), label_columns=['electricity'])\n",
    "seq_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the PV data\n",
    "\n",
    "column_indices = {name: i for i, name in enumerate(pv_processed.columns)}\n",
    "\n",
    "n = len(pv_processed)\n",
    "train_df_pv = pv_processed[0:int(n*0.7)]\n",
    "val_df_pv = pv_processed[int(n*0.7):int(n*0.9)]\n",
    "test_df_pv = pv_processed[int(n*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Forecast data\n",
    "# Careful: split must be done for 24h data\n",
    "\n",
    "column_indices = {name: i for i, name in enumerate(forecast_processed.columns)}\n",
    "\n",
    "n = len(forecast_processed)\n",
    "train_df_forecast = forecast_processed[0:int(n*0.7 / 24) * 24]\n",
    "val_df_forecast = forecast_processed[int(n*0.7 / 24) * 24:int(n*0.9 / 24) * 24]\n",
    "test_df_forecast = forecast_processed[int(n*0.9 / 24) * 24:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Baseline metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 Compute regression metrics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_regression_metrics(model, dataloader):\n",
    "    y_preds = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            outputs = model.forward(inputs)\n",
    "            y_preds.append(outputs.cpu())\n",
    "            labels.append(targets.cpu())\n",
    "\n",
    "    y_preds = torch.cat(y_preds)\n",
    "    labels = torch.cat(labels)\n",
    "\n",
    "    # Compute metrics\n",
    "    mse = torch.mean((y_preds - labels) ** 2)\n",
    "    rmse = mse ** 0.5\n",
    "    mae = torch.mean(abs(y_preds - labels))\n",
    "    r2 = 1 - torch.sum((y_preds - labels) ** 2) / torch.sum(y_preds - torch.mean(labels) ** 2)\n",
    "\n",
    "    return {\"mse\": mse, \"rmse\": rmse, \"mae\": mae, \"r2\": r2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 Photovoltaic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_pv = SequenceGenerator(input_width= INPUT_WIDTH,\n",
    "                              label_width= LABEL_WIDTH,\n",
    "                              shift= SHIFT,\n",
    "                              number_sequences= 10_000,\n",
    "                              train_df= train_df_pv[num_cols_pv],\n",
    "                              val_df= val_df_pv[num_cols_pv],\n",
    "                              test_df= test_df_pv[num_cols_pv],\n",
    "                              label_columns= ['electricity'])\n",
    "\n",
    "train_dataset_pv = sequences_pv.train\n",
    "val_dataset_pv = sequences_pv.val\n",
    "test_dataset_pv = sequences_pv.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselinePV(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[:, 12:-12, 4]\n",
    "        return x.view(len(x), -1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader_pv = DataLoader(test_dataset_pv, batch_size= 32, shuffle= True)\n",
    "baseline_pv = compute_regression_metrics(BaselinePV(), test_loader_pv)\n",
    "\n",
    "print(\"Baseline Metrics:\", baseline_pv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 Forecast data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stick day_ahead and day_before electricity data to the forecast data\n",
    "pv_data['local_time'] = pd.to_datetime(pv_data['local_time'], utc= True)\n",
    "merged_forecast_pv = pd.merge(pv_data[['local_time', 'electricity']], forecast_clean, left_on='local_time', right_on= 'prediction_utc_time', how='inner')\n",
    "merged_forecast_pv = merged_forecast_pv.rename(columns= {'electricity': 'electricity_day_ahead'})\n",
    "\n",
    "merged_forecast_pv['baseline_day_utc_time'] = merged_forecast_pv['prediction_utc_time'] - dt.timedelta(days= 2)\n",
    "merged_forecast_pv = pd.merge(pv_data[['local_time', 'electricity']], merged_forecast_pv, left_on='local_time', right_on= 'baseline_day_utc_time', how='inner')\n",
    "merged_forecast_pv = merged_forecast_pv.rename(columns= {'electricity': 'electricity_day_before'})\n",
    "\n",
    "time_cols_mask = [col for col in merged_forecast_pv.columns if 'time' not in col]\n",
    "\n",
    "baseline_forecast_df = merged_forecast_pv[time_cols_mask]\n",
    "baseline_forecast_dataset = SequenceForecastDataset(df= baseline_forecast_df,\n",
    "                                                label_columns= ['electricity_day_ahead'])\n",
    "display(baseline_forecast_dataset.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Forecast data\n",
    "# Careful: split must be done for 24h data\n",
    "\n",
    "column_indices = {name: i for i, name in enumerate(baseline_forecast_df.columns)}\n",
    "\n",
    "n = len(baseline_forecast_df)\n",
    "train_baseline_df_forecast = baseline_forecast_df[0:int(n*0.7 / 24) * 24]\n",
    "val_baseline_df_forecast = baseline_forecast_df[int(n*0.7 / 24) * 24:int(n*0.9 / 24) * 24]\n",
    "test_df_baseline_forecast = baseline_forecast_df[int(n*0.9 / 24) * 24:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_baseline_dataset = SequenceForecastDataset(df= test_df_baseline_forecast,\n",
    "                                            label_columns= ['electricity_day_ahead'])\n",
    "\n",
    "test_baseline_loader_forecast = DataLoader(test_baseline_dataset, batch_size= 32, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineForecast(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[:, :, 0]\n",
    "        return x.view(len(x), -1, 1)\n",
    "\n",
    "baseline_forecast = compute_regression_metrics(BaselineForecast(), test_baseline_loader_forecast)\n",
    "print(\"Baseline Metrics:\", baseline_forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.4 Visualize the baseline prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_n = 3\n",
    "seq_number = np.random.randint(0, 10000, max_n)\n",
    "\n",
    "### Plot #######################\n",
    "########################################\n",
    "fig, axs = plt.subplots(nrows= max_n, ncols= 1, figsize=(16,7), sharex= True, sharey= True)\n",
    "\n",
    "for n, ax in enumerate(axs):\n",
    "\n",
    "    ax.plot(sequences_pv.input_indices,test_dataset_pv[seq_number[n]][0][:,4], label='Inputs', marker='.')#, zorder=-10, c='b')\n",
    "    ax.scatter(sequences_pv.label_indices, test_dataset_pv[seq_number[n]][1][:,0], edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "    ax.scatter(sequences_pv.label_indices, test_dataset_pv[seq_number[n]][0][12:-12,4], marker='X', edgecolors='k', label='Predictions',c='#ff7f0e', s=64)\n",
    "\n",
    "    ax.vlines(11, ymin=0, ymax=1, color='k', linewidth=0.7, linestyle='--')\n",
    "    ax.vlines(35, ymin=0, ymax=1, color='k', linewidth=0.7, linestyle='--')\n",
    "    ax.set_ylim(-0.1, 1.1)\n",
    "    ax.set_xlim(-1,84)\n",
    "\n",
    "    # ax.set_axis_off()\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylabel('PV capacity / (a.u.)')\n",
    "\n",
    "    x_labels = np.arange(sequences_pv.total_window_size, step=12)\n",
    "    ax.set_xticks([i - 1 for i in x_labels])\n",
    "    ax.set_xticklabels([])\n",
    "\n",
    "    for value in x_labels:\n",
    "        ax.text(x= value-1, y=-0.2, s= f\"{value}h\", va='center', ha='center')\n",
    "\n",
    "    ax.legend(loc=(0.6, 0.5))\n",
    "\n",
    "plt.text(x= 42, y=-0.3, s= f\"Time (h)\", va='center', ha='center')\n",
    "\n",
    "text = 'Baseline Predicitions Interval'\n",
    "fig.text(\n",
    "    0.35, 0.84,\n",
    "    text,\n",
    "    ha='center',\n",
    "    fontsize=14,\n",
    "    c='#ff7f0e'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Prepare the datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_pv = SequenceGenerator(input_width= INPUT_WIDTH,\n",
    "                              label_width= LABEL_WIDTH,\n",
    "                              shift= SHIFT,\n",
    "                              number_sequences= 10_000,\n",
    "                              train_df= train_df_pv[num_cols_pv],\n",
    "                              val_df= val_df_pv[num_cols_pv],\n",
    "                              test_df= test_df_pv[num_cols_pv],\n",
    "                              label_columns= ['electricity'])\n",
    "\n",
    "train_dataset_pv = sequences_pv.train\n",
    "val_dataset_pv = sequences_pv.val\n",
    "test_dataset_pv = sequences_pv.test\n",
    "\n",
    "train_loader_pv = DataLoader(train_dataset_pv, batch_size= 32, shuffle= True)\n",
    "val_loader_pv = DataLoader(val_dataset_pv, batch_size= 32, shuffle= True)\n",
    "test_loader_pv = DataLoader(test_dataset_pv, batch_size= 32, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_forecast = SequenceForecastDataset(df= train_df_forecast.select_dtypes(include=np.number), label_columns=['electricity'])\n",
    "val_dataset_forecast = SequenceForecastDataset(df= val_df_forecast.select_dtypes(include=np.number), label_columns=['electricity'])\n",
    "test_dataset_forecast = SequenceForecastDataset(df= test_df_forecast.select_dtypes(include=np.number), label_columns=['electricity'])\n",
    "\n",
    "train_loader_forecast = DataLoader(train_dataset_forecast, batch_size= 16, shuffle= True)\n",
    "val_loader_forecast = DataLoader(val_dataset_forecast, batch_size= 16, shuffle= True)\n",
    "test_loader_forecast = DataLoader(test_dataset_forecast, batch_size= 16, shuffle= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Simple LSTM and RNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, p: int):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.n_features = p\n",
    "        self.lstm = nn.LSTM(p, 24, batch_first= True)\n",
    "        self.linear = nn.Linear(24,24)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = F.tanh(x)\n",
    "        x = self.linear(x[:,-1,:])\n",
    "        return x.view(len(x), -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, p: int):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.n_features = p\n",
    "        self.lstm = nn.RNN(p, 24, batch_first= True)\n",
    "        self.linear = nn.Linear(24,24)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = F.tanh(x)\n",
    "        x = self.linear(x[:,-1,:])\n",
    "        return x.view(len(x), -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float(\"inf\")\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_forecast.__len__(), val_loader_forecast.__len__(), test_loader_forecast.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Data ##############\n",
    "dataset = 'pv'  # ['pv', 'forecast']\n",
    "\n",
    "########## Parameters #########\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "EPOCHS = 100\n",
    "DATASET = dataset\n",
    "patience = 5\n",
    "\n",
    "if DATASET == \"pv\":\n",
    "    train_dataset = sequences_pv.train\n",
    "    val_dataset = sequences_pv.val\n",
    "    test_dataset = sequences_pv.test\n",
    "    n_features = train_dataset.tensors[0].shape[-1]\n",
    "else:\n",
    "    train_dataset = train_dataset_forecast\n",
    "    val_dataset = val_dataset_forecast\n",
    "    test_dataset = test_dataset_forecast\n",
    "    n_features = train_dataset.example[0].shape[-1]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = RNNModel(p=n_features)\n",
    "loss_fn = nn.MSELoss()\n",
    "optim = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "def training_one_epoch(model, train_dataloader, val_dataloader):\n",
    "    size = len(train_dataloader.dataset)\n",
    "    running_loss = 0\n",
    "    # earlystopping = 0\n",
    "    for batch, data in enumerate(train_dataloader):\n",
    "        X, y = data\n",
    "        output = model(X)\n",
    "        loss = loss_fn(output, y)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        running_loss += loss.item()\n",
    "        mae = torch.mean(abs(output - y))\n",
    "\n",
    "        if batch % 10 == 9:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"\\tloss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = []\n",
    "        labels = []\n",
    "        vsize = len(val_dataloader.dataset)\n",
    "        running_vloss = 0.0\n",
    "\n",
    "        # In evaluation mode some model specific operations can be omitted\n",
    "        #  -> eg. dropout layer\n",
    "        # Switching to evaluation mode, eg. turning off regularisation\n",
    "        model.train(False)\n",
    "        for j, vdata in enumerate(val_dataloader):\n",
    "            vinputs, vlabels = vdata\n",
    "            voutputs = model(vinputs)\n",
    "            outputs.append(voutputs)\n",
    "            labels.append(vlabels)\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            vmae = torch.mean(abs(voutputs - vlabels))\n",
    "            running_vloss += vloss.item()\n",
    "\n",
    "            if j % 10 == 9:\n",
    "                vloss, vcurrent = vloss.item(), j * batch_size + len(vinputs)\n",
    "                print(f\"\\tval loss: {vloss:>7f}  [{vcurrent:>5d}/{vsize:>5d}]\")\n",
    "\n",
    "        model.train(True)\n",
    "\n",
    "    return loss, vloss, mae, vmae\n",
    "\n",
    "history = defaultdict(list)\n",
    "early_stopping = EarlyStopper(patience=patience)\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    print(f\"Epoch {epoch + 1}:\")\n",
    "    model.train()\n",
    "    loss, vloss, mae, vmae = training_one_epoch(model, train_loader, val_loader)\n",
    "\n",
    "    history[\"loss\"].append(loss.detach())\n",
    "    history[\"val_loss\"].append(vloss)\n",
    "    history[\"mae\"].append(mae.detach())\n",
    "    history[\"val_mae\"].append(vmae)\n",
    "\n",
    "    if early_stopping.early_stop(vloss):\n",
    "        print(f\"Early stopping at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "\n",
    "metrics = compute_regression_metrics(model, test_loader)\n",
    "\n",
    "val_mae = np.min(history[\"val_mae\"])\n",
    "print(\"Regression Metrics:\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Plot Loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_mae(history):\n",
    "    # Setting figures\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(13,4))\n",
    "\n",
    "    # Create the plots\n",
    "    ax1.plot(history['loss'])\n",
    "    ax1.plot(history['val_loss'])\n",
    "    ax2.plot(history['mae'])\n",
    "    ax2.plot(history['val_mae'])\n",
    "\n",
    "    # Set titles and labels\n",
    "    ax1.set_title('Model loss')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "\n",
    "    ax2.set_title('Mean Absolute Error')\n",
    "    ax2.set_ylabel('MAE')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "\n",
    "    # Set limits for y-axes\n",
    "    # ax1.set_ylim(ymin=0, ymax=20)\n",
    "    # ax2.set_ylim(ymin=0, ymax=200)\n",
    "\n",
    "    # Generate legends\n",
    "    ax1.legend(['Train', 'Validation'], loc='best')\n",
    "    ax2.legend(['Train', 'Validation'], loc='best')\n",
    "\n",
    "    # Show grids\n",
    "    ax1.grid(axis=\"x\",linewidth=0.5)\n",
    "    ax1.grid(axis=\"y\",linewidth=0.5)\n",
    "\n",
    "    ax2.grid(axis=\"x\",linewidth=0.5)\n",
    "    ax2.grid(axis=\"y\",linewidth=0.5)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_mae(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "compute_regression_metrics(model, test_loader_pv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Make a prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a prediction with our model. <br>\n",
    "We take as an input the day we want to predict the PV production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "input_pred = \"2022-07-06\"\n",
    "\n",
    "# Compute the necessary datetime objects for the BQ querying\n",
    "dt_day_ahead_begin = pd.to_datetime(input_pred, utc=True)\n",
    "dt_day_ahead_end = dt_day_ahead_begin + np.timedelta64(23, 'h')\n",
    "\n",
    "dt_pv_data_begin = dt_day_ahead_begin - np.timedelta64(2, 'D')\n",
    "dt_pv_data_end = dt_day_ahead_end -np.timedelta64(2, 'D')\n",
    "\n",
    "dt_noon_weather_forecast = dt_day_ahead_begin - np.timedelta64(12, 'h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the data from BigQuery directly.<br>\n",
    "This way we work in real-life scenario: we have a raw weather forecast for the day ahead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set-up the BQ query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCP_PROJECT = os.environ.get(\"GCP_PROJECT\")\n",
    "BQ_DATASET = os.environ.get(\"BQ_DATASET\")\n",
    "\n",
    "def query_bq_data(\n",
    "        gcp_project:str,\n",
    "        query:str,\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieve `query` data from BigQuery\n",
    "    \"\"\"\n",
    "\n",
    "    print(Fore.BLUE + \"\\nLoad data from BigQuery server...\" + Style.RESET_ALL)\n",
    "    client = bigquery.Client(project=gcp_project)\n",
    "    query_job = client.query(query)\n",
    "    result = query_job.result()\n",
    "    df = result.to_dataframe()\n",
    "\n",
    "    print(f\"✅ Data loaded, with shape {df.shape}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query raw PV data from BUCKET BigQuery\n",
    "query_pv = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM {GCP_PROJECT}.{BQ_DATASET}.raw_pv\n",
    "    WHERE _0 BETWEEN {int(dt_pv_data_begin.timestamp()) * 1000}\n",
    "                AND {int(dt_noon_weather_forecast.timestamp()) * 1000}\n",
    "    ORDER BY _0\n",
    "\"\"\"\n",
    "\n",
    "# Retrieve data from BigQuery\n",
    "data_pv_query = query_bq_data(\n",
    "    query=query_pv,\n",
    "    gcp_project=GCP_PROJECT\n",
    ")\n",
    "\n",
    "# Query raw historical weather forecast data from BigQuery\n",
    "query_forecast = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM {GCP_PROJECT}.{BQ_DATASET}.raw_weather_forecast\n",
    "    WHERE forecast_dt_unixtime = {int(dt_noon_weather_forecast.timestamp())}\n",
    "    ORDER BY forecast_dt_unixtime, slice_dt_unixtime\n",
    "\"\"\"\n",
    "\n",
    "data_forecast_query = query_bq_data(\n",
    "    query=query_forecast,\n",
    "    gcp_project=GCP_PROJECT\n",
    ")\n",
    "\n",
    "# fix issue with column name in BigQuery (the issue is just in the notebook)\n",
    "try:\n",
    "    assert data_forecast_query.shape[1] == forecast_data.shape[1]\n",
    "    data_forecast_query.columns = forecast_data.columns\n",
    "except AssertionError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean and preprocess the PV data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pv_query[\"local_time\"] = pd.to_datetime(data_pv_query[\"local_time\"], utc=True)\n",
    "\n",
    "# Clean data for the first 24h only\n",
    "timestamp_pv_data_end = int(dt_pv_data_end.timestamp()) * 1000\n",
    "data_pv_processed = preprocess_pv.transform(data_pv_query.query(\"_0 <= @timestamp_pv_data_end\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pv_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean and preprocess the Weather forecast data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "data_forecast_clean = clean_forecast_data(data_forecast_query)\n",
    "\n",
    "data_forecast_processed = forecast_transformer.transform(data_forecast_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_forecast_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == \"pv\":\n",
    "    # model = load_model(forecast_features=False)\n",
    "    assert model is not None\n",
    "    X = data_pv_processed.select_dtypes(include=np.number)\n",
    "    X = np.expand_dims(X.values, axis=0)\n",
    "\n",
    "else:\n",
    "    # model = load_model()\n",
    "    assert model is not None\n",
    "    X = data_forecast_processed.select_dtypes(include=np.number)\n",
    "    X = np.expand_dims(X.values, axis=0)\n",
    "\n",
    "    X_tensor = torch.tensor(X, dtype= torch.float32)\n",
    "X.shape, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "X_tensor = torch.tensor(X, dtype= torch.float32)\n",
    "\n",
    "\n",
    "y_pred = model.forward(X_tensor)\n",
    "y_pred_df = pd.DataFrame(y_pred.detach().numpy()[0], columns=[\"pred\"])\n",
    "y_pred_df[\"utc_time\"] = data_forecast_clean['prediction_utc_time']\n",
    "y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df[\"utc_time\"] = data_forecast_clean['prediction_utc_time']\n",
    "y_pred_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you start TensorBoard at the command line and open it in a new browser tab (usually at [localhost:6006](localhost:6006)), you should see the image grid under the IMAGES tab.\n",
    "\n",
    "## Graphing Scalars to Visualize Training\n",
    "\n",
    "TensorBoard is useful for tracking the progress and efficacy of your training. Below, we'll run a training loop, track some metrics, and save the data for TensorBoard's consumption.\n",
    "\n",
    "Let's define a model to categorize our image tiles, and an optimizer and loss function for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# # Default log_dir argument is \"runs\" - but it's good to be specific\n",
    "# # torch.utils.tensorboard.SummaryWriter is imported above\n",
    "# writer = SummaryWriter()\n",
    "# # writer.close()\n",
    "\n",
    "# # Write image data to TensorBoard log dir\n",
    "# # writer.add_image('Four Fashion-MNIST Images', img_grid)\n",
    "# # writer.flush()\n",
    "\n",
    "# # To view, start TensorBoard on the command line with:\n",
    "# #   tensorboard --logdir=runs\n",
    "# # ...and open a browser tab to http://localhost:6006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############# Data ##############\n",
    "# dataset = 'forecast'  # ['pv', 'forecast']\n",
    "\n",
    "# ########## Parameters #########\n",
    "# batch_size = 32\n",
    "# learning_rate = 1e-3\n",
    "# epochs = 10\n",
    "\n",
    "\n",
    "# if dataset == 'pv':\n",
    "#     train_dataset = train_dataset_pv\n",
    "#     test_dataset = test_dataset_pv\n",
    "#     n_features = train_dataset_pv.tensors[0].shape[-1]\n",
    "# else:\n",
    "#     train_dataset = train_dataset_forecast\n",
    "#     test_dataset = test_dataset_forecast\n",
    "#     n_features = train_dataset.example[0].shape[-1]\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size= batch_size, shuffle= True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size= batch_size, shuffle= True)\n",
    "\n",
    "# model = LSTMModel(p = n_features)\n",
    "# loss_fn = nn.MSELoss()\n",
    "# optim = torch.optim.Adam(params= model.parameters(), lr= learning_rate)\n",
    "\n",
    "# print(len(val_loader_forecast))\n",
    "# for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     for i, data in enumerate(train_loader_forecast, 0):\n",
    "#         # basic training loop\n",
    "#         inputs, labels = data\n",
    "#         optim.zero_grad()\n",
    "#         outputs = model(inputs)\n",
    "#         loss = loss_fn(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optim.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "#         if i % 8 == 7:    # Every 1000 mini-batches...\n",
    "#             print('Batch {}'.format(i + 1))\n",
    "#             # Check against the validation set\n",
    "#             running_vloss = 0.0\n",
    "\n",
    "#             # In evaluation mode some model specific operations can be omitted eg. dropout layer\n",
    "#             model.train(False) # Switching to evaluation mode, eg. turning off regularisation\n",
    "#             for j, vdata in enumerate(val_loader_forecast, 0):\n",
    "#                 vinputs, vlabels = vdata\n",
    "#                 voutputs = model(vinputs)\n",
    "#                 vloss = loss_fn(voutputs, vlabels)\n",
    "#                 running_vloss += vloss.item()\n",
    "#             model.train(True) # Switching back to training mode, eg. turning on regularisation\n",
    "\n",
    "#             avg_loss = running_loss / 1000\n",
    "#             avg_vloss = running_vloss / len(val_loader_forecast)\n",
    "\n",
    "#             # Log the running loss averaged per batch\n",
    "#             writer.add_scalars('Training vs. Validation Loss',\n",
    "#                             { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "#                             epoch * len(train_loader_forecast) + i)\n",
    "\n",
    "#             running_loss = 0.0\n",
    "# print('Finished Training')\n",
    "\n",
    "# writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switch to your open TensorBoard and have a look at the SCALARS tab.\n",
    "\n",
    "## Visualizing Your Model\n",
    "TensorBoard can also be used to examine the data flow within your model. To do this, call the `add_graph()` method with a model and sample input. When you open "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Again, grab a single mini-batch of images\n",
    "# dataiter = iter(train_loader_forecast)\n",
    "# inputs, labels = next(dataiter)\n",
    "\n",
    "# # add_graph() will trace the sample input through your model,\n",
    "# # and render it as a graph.\n",
    "# writer.add_graph(model, inputs)\n",
    "# writer.flush()\n",
    "# writer.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_pv, baseline_forecast"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, MinMaxScaler, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, make_union\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
    "\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Collected data**\n",
    "\n",
    "#### üìå Tempelhofer Feld, Berlin, Germany"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe src=\"https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d26842.24808091901!2d13.400488957085193!3d52.477045271245125!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x47a84fe8f7d899eb%3A0x88898e99acbb718b!2sTempelhofer%20Feld!5e0!3m2!1sen!2sde!4v1736167638260!5m2!1sen!2sde\" width=\"1200\" height=\"450\" style=\"border:0;\" allowfullscreen=\"\" loading=\"lazy\" referrerpolicy=\"no-referrer-when-downgrade\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ‚òÄÔ∏è 1. Historical photovoltaic production data\n",
    "* 42 years (1980 - 2022)\n",
    "* hourly resolution\n",
    "* synthetic data derived from measured irradiance data\n",
    "* ~ 400 000 data points\n",
    "* source: [Renewables.ninja](https://www.renewables.ninja/about \"Renewables.ninja\")\n",
    "\n",
    "### üå°Ô∏è 2. Historical weather forecast data\n",
    "* 6 1/2 years (Oct 2017 - Mar 2024)\n",
    "* 4 model recalculation cycles available for each day (updated at: 00:00 UTC, 06:00 UTC, 12:00 UTC, 18:00 UTC)\n",
    "* Each 16 days forecast in hourly resolution\n",
    "* ~ 3.3 M rows - 24 Features\n",
    "* source: [OpenWeather](https://openweathermap.org/api/history-forecast-bulk \"History-forecast-bulk\") (40 ‚Ç¨)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_data = pd.read_csv('./data/1980-2022_pv.csv')\n",
    "forecast_data = pd.read_csv('./data/openweather_history_bulk_forecast_tempelhof.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Check the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Photovolatic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 376944 entries, 0 to 376943\n",
      "Data columns (total 8 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   Unnamed: 0.1        376944 non-null  int64  \n",
      " 1   Unnamed: 0          376944 non-null  int64  \n",
      " 2   local_time          376944 non-null  object \n",
      " 3   electricity         376944 non-null  float64\n",
      " 4   irradiance_direct   376944 non-null  float64\n",
      " 5   irradiance_diffuse  376944 non-null  float64\n",
      " 6   temperature         376944 non-null  float64\n",
      " 7   source              376944 non-null  object \n",
      "dtypes: float64(4), int64(2), object(2)\n",
      "memory usage: 23.0+ MB\n"
     ]
    }
   ],
   "source": [
    "pv_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0.1          0\n",
       "Unnamed: 0            0\n",
       "local_time            0\n",
       "electricity           0\n",
       "irradiance_direct     0\n",
       "irradiance_diffuse    0\n",
       "temperature           0\n",
       "source                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pv_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0.1          0\n",
       "Unnamed: 0            0\n",
       "local_time            0\n",
       "electricity           0\n",
       "irradiance_direct     0\n",
       "irradiance_diffuse    0\n",
       "temperature           0\n",
       "source                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pv_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <td>376944.0</td>\n",
       "      <td>4.382576e+03</td>\n",
       "      <td>2.530581e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.191000e+03</td>\n",
       "      <td>4.383000e+03</td>\n",
       "      <td>6.574000e+03</td>\n",
       "      <td>8.783000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <td>376944.0</td>\n",
       "      <td>9.940302e+11</td>\n",
       "      <td>3.917322e+11</td>\n",
       "      <td>3.155328e+11</td>\n",
       "      <td>6.547815e+11</td>\n",
       "      <td>9.940302e+11</td>\n",
       "      <td>1.333279e+12</td>\n",
       "      <td>1.672528e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electricity</th>\n",
       "      <td>376944.0</td>\n",
       "      <td>1.345303e-01</td>\n",
       "      <td>2.148500e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>9.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>irradiance_direct</th>\n",
       "      <td>376944.0</td>\n",
       "      <td>1.068965e-01</td>\n",
       "      <td>2.120717e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.700000e-02</td>\n",
       "      <td>1.016000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>irradiance_diffuse</th>\n",
       "      <td>376944.0</td>\n",
       "      <td>5.566084e-02</td>\n",
       "      <td>7.542225e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.000000e-03</td>\n",
       "      <td>1.020000e-01</td>\n",
       "      <td>3.580000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temperature</th>\n",
       "      <td>376944.0</td>\n",
       "      <td>9.219034e+00</td>\n",
       "      <td>9.013729e+00</td>\n",
       "      <td>-2.745800e+01</td>\n",
       "      <td>2.217000e+00</td>\n",
       "      <td>8.863000e+00</td>\n",
       "      <td>1.591600e+01</td>\n",
       "      <td>3.875600e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       count          mean           std           min  \\\n",
       "Unnamed: 0.1        376944.0  4.382576e+03  2.530581e+03  0.000000e+00   \n",
       "Unnamed: 0          376944.0  9.940302e+11  3.917322e+11  3.155328e+11   \n",
       "electricity         376944.0  1.345303e-01  2.148500e-01  0.000000e+00   \n",
       "irradiance_direct   376944.0  1.068965e-01  2.120717e-01  0.000000e+00   \n",
       "irradiance_diffuse  376944.0  5.566084e-02  7.542225e-02  0.000000e+00   \n",
       "temperature         376944.0  9.219034e+00  9.013729e+00 -2.745800e+01   \n",
       "\n",
       "                             25%           50%           75%           max  \n",
       "Unnamed: 0.1        2.191000e+03  4.383000e+03  6.574000e+03  8.783000e+03  \n",
       "Unnamed: 0          6.547815e+11  9.940302e+11  1.333279e+12  1.672528e+12  \n",
       "electricity         0.000000e+00  0.000000e+00  2.000000e-01  9.000000e-01  \n",
       "irradiance_direct   0.000000e+00  0.000000e+00  7.700000e-02  1.016000e+00  \n",
       "irradiance_diffuse  0.000000e+00  4.000000e-03  1.020000e-01  3.580000e-01  \n",
       "temperature         2.217000e+00  8.863000e+00  1.591600e+01  3.875600e+01  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pv_data.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 check for time gaps in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "376944"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of rows with unique values in our data\n",
    "local_time_series = pd.to_datetime(pv_data['local_time'], utc= True)\n",
    "\n",
    "unique_timestamps_pv_dataset = local_time_series.nunique()\n",
    "unique_timestamps_pv_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "376944"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of rows with unique values of a synthetic data with the same range\n",
    "min_date = local_time_series.min()\n",
    "max_date = local_time_series.max()\n",
    "unique_timestamps_pv_dataset_theoretical = pd.date_range(start= min_date, end= max_date, freq=dt.timedelta(hours=1.0)).nunique()\n",
    "unique_timestamps_pv_dataset_theoretical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all good! there are no gaps in your dataset...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    assert unique_timestamps_pv_dataset == unique_timestamps_pv_dataset_theoretical\n",
    "    print('all good! there are no gaps in your dataset...')\n",
    "except AssertionError:\n",
    "    print('there are gaps in your dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Photovolatic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>local_time</th>\n",
       "      <th>electricity</th>\n",
       "      <th>irradiance_direct</th>\n",
       "      <th>irradiance_diffuse</th>\n",
       "      <th>temperature</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>315532800000</td>\n",
       "      <td>1980-01-01 01:00:00+01:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.296</td>\n",
       "      <td>data/pv_data/1980_pv.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>315536400000</td>\n",
       "      <td>1980-01-01 02:00:00+01:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.216</td>\n",
       "      <td>data/pv_data/1980_pv.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>315540000000</td>\n",
       "      <td>1980-01-01 03:00:00+01:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.005</td>\n",
       "      <td>data/pv_data/1980_pv.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>315543600000</td>\n",
       "      <td>1980-01-01 04:00:00+01:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.063</td>\n",
       "      <td>data/pv_data/1980_pv.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>315547200000</td>\n",
       "      <td>1980-01-01 05:00:00+01:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.227</td>\n",
       "      <td>data/pv_data/1980_pv.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1    Unnamed: 0                 local_time  electricity  \\\n",
       "0             0  315532800000  1980-01-01 01:00:00+01:00          0.0   \n",
       "1             1  315536400000  1980-01-01 02:00:00+01:00          0.0   \n",
       "2             2  315540000000  1980-01-01 03:00:00+01:00          0.0   \n",
       "3             3  315543600000  1980-01-01 04:00:00+01:00          0.0   \n",
       "4             4  315547200000  1980-01-01 05:00:00+01:00          0.0   \n",
       "\n",
       "   irradiance_direct  irradiance_diffuse  temperature  \\\n",
       "0                0.0                 0.0       -1.296   \n",
       "1                0.0                 0.0       -1.216   \n",
       "2                0.0                 0.0       -1.005   \n",
       "3                0.0                 0.0       -1.063   \n",
       "4                0.0                 0.0       -1.227   \n",
       "\n",
       "                     source  \n",
       "0  data/pv_data/1980_pv.csv  \n",
       "1  data/pv_data/1980_pv.csv  \n",
       "2  data/pv_data/1980_pv.csv  \n",
       "3  data/pv_data/1980_pv.csv  \n",
       "4  data/pv_data/1980_pv.csv  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pv_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_encoding(X: pd.DataFrame, column_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert local time into cyclic features to feed significant signal\n",
    "    in ML / DL algorithm\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): datafrane to transform\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with 4 addtional features per time column\n",
    "    \"\"\"\n",
    "    X = X.copy()\n",
    "\n",
    "    local_time = pd.to_datetime(X.pop(column_name), utc= True)\n",
    "    timestamp_s = local_time.map(pd.Timestamp.timestamp)\n",
    "\n",
    "\n",
    "    day = 24*60*60\n",
    "    year = (365.2425)*day\n",
    "    X['day_sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
    "    X['day_cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
    "    X['year_sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
    "    X['year_cos'] = np.cos(timestamp_s * (2 * np.pi / year))\n",
    "\n",
    "    return X.iloc[:,-4:]\n",
    "\n",
    "\n",
    "time_encoder = FunctionTransformer(time_encoding, kw_args={'column_name': 'local_time'})\n",
    "\n",
    "pv_transformer = ColumnTransformer(\n",
    "    [\n",
    "        ('Time Encoder', time_encoder, ['local_time']),\n",
    "        ('Passthrough', 'passthrough', ['local_time', 'electricity' ]),\n",
    "    ],\n",
    "    remainder='drop'\n",
    ").set_output(transform='pandas')\n",
    "\n",
    "preprocess_pv = Pipeline([('Time Features', pv_transformer)])\n",
    "pv_processed = preprocess_pv.fit_transform(pv_data)\n",
    "\n",
    "pv_processed.columns = [column.split('__')[1] for column in pv_processed.columns]\n",
    "pv_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_pv_df = pd.melt(pv_processed.iloc[:,:])\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "sns.violinplot(data=melted_pv_df.sample(10_000), x='variable', y='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_indices = {name:i for i, name in enumerate(pv_processed.columns)}\n",
    "column_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Weather forecast data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(df: pd.DataFrame, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reduces size of dataframe by downcasting numerical columns\n",
    "    \"\"\"\n",
    "    input_size = df.memory_usage(index=True).sum() / 1024\n",
    "    print(\"dataframe size: \", round(input_size,2), 'kB')\n",
    "\n",
    "    in_size = df.memory_usage(index=True).sum()\n",
    "    for type in [\"float\", \"integer\"]:\n",
    "        l_cols = list(df.select_dtypes(include=type))\n",
    "        for col in l_cols:\n",
    "            df[col] = pd.to_numeric(df[col], downcast=type)\n",
    "    out_size = df.memory_usage(index=True).sum()\n",
    "    ratio = (1 - round(out_size / in_size, 2)) * 100\n",
    "\n",
    "    print(\"optimized size by {} %\".format(round(ratio,2)))\n",
    "    print(\"new dataframe size: \", round(out_size / 1024,2), \" kB\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def clean_forecast_data(forecast_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Initial has 3.3 M entries (everyday: 4 forecasts of 16 days ahead)\n",
    "    Cleaning it to: - 1 forecast perday (at 12:00)\n",
    "                    - 48 hours a day\n",
    "                    - right now hardcoded to match last forecast day with\n",
    "                     last day of PV data\n",
    "    \"\"\"\n",
    "    df = compress(forecast_df)\n",
    "    df = df.drop(columns=['lat', 'lon',\n",
    "                          'forecast dt iso',\n",
    "                          'slice dt iso'])\n",
    "\n",
    "    df.rename(columns={'forecast dt unixtime':'utc_time',\n",
    "                        'slice dt unixtime':'prediction_utc_time'},\n",
    "                        inplace=True)\n",
    "\n",
    "    # df['utc_time'] = df['utc_time'].str.replace('+0000 UTC', '')\n",
    "    # df['prediction_utc_time'] = df['prediction_utc_time'].str.replace('+0000 UTC', '')\n",
    "\n",
    "    df['utc_time'] = pd.to_datetime(df['utc_time'], unit= 's', utc= True)\n",
    "    df['prediction_utc_time'] = pd.to_datetime(df['prediction_utc_time'], unit= 's', utc= True)\n",
    "\n",
    "    # # get only 1 forecast per day\n",
    "    df = df[df['utc_time'].dt.hour == 12]\n",
    "\n",
    "    unique_dates = df['utc_time'].unique()\n",
    "\n",
    "    # reduce to 24h of weather forecast (from 00:00 to 23:00 each day)\n",
    "    df_revised = []\n",
    "    for date in unique_dates:\n",
    "        data = df[(df['utc_time'] == date)].iloc[12:36]\n",
    "        df_revised.append(data)\n",
    "\n",
    "    df_revised_ordered = pd.concat(df_revised, ignore_index=True)\n",
    "\n",
    "    # # hard code the end date to match wiht PV data\n",
    "    # processed_df = df_revised_ordered[df_revised_ordered['prediction_utc_time'] <= '2022-12-31 23:00:00']\n",
    "\n",
    "    return df_revised_ordered\n",
    "\n",
    "forecast_clean = clean_forecast_data(forecast_data)\n",
    "forecast_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stick electricity data to the forecast data\n",
    "pv_data['local_time'] = pd.to_datetime(pv_data['local_time'], utc= True)\n",
    "merged_forecast_pv = pd.merge(pv_data[['local_time', 'electricity']], forecast_clean, left_on='local_time', right_on= 'prediction_utc_time', how='inner')\n",
    "merged_forecast_pv.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_encoding(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert local time into cyclic features to feed significant signal\n",
    "    in ML / DL algorithm\n",
    "\n",
    "    Args:\n",
    "    X: datafrane to transform\n",
    "\n",
    "    Output:\n",
    "    DataFrame with 4 addtional features per time column\n",
    "    \"\"\"\n",
    "    X = X.copy()\n",
    "\n",
    "    date_time = pd.to_datetime(X.pop('prediction_utc_time'), utc= True)\n",
    "    timestamp_s = date_time.map(pd.Timestamp.timestamp)\n",
    "\n",
    "    day = 24*60*60\n",
    "    year = (365.2425)*day\n",
    "    X['forecast_day_sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
    "    X['forecast_day_cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
    "    X['forecast_year_sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
    "    X['forecast_year_cos'] = np.cos(timestamp_s * (2 * np.pi / year))\n",
    "\n",
    "    return X.iloc[:,-4:]\n",
    "\n",
    "def wind_encoding(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert local time into cyclic features to feed significant signal\n",
    "    in ML / DL algorithm\n",
    "\n",
    "    Args:\n",
    "    X: datafrane to transform\n",
    "\n",
    "    Output:\n",
    "    DataFrame with 4 addtional features per time column\n",
    "    \"\"\"\n",
    "    X = X.copy()\n",
    "\n",
    "    # Process wind fratures\n",
    "    wind_speed = X.pop('wind_speed')\n",
    "\n",
    "    # Convert to radians.\n",
    "    wind_rad = X.pop('wind_deg')*np.pi / 180\n",
    "\n",
    "    # Calculate the wind x and y components\n",
    "    X['Wx'] = wind_speed*np.cos(wind_rad)\n",
    "    X['Wy'] = wind_speed*np.sin(wind_rad)\n",
    "\n",
    "    # Standardize the components\n",
    "    X['Wx'] = (X['Wx'] - X['Wx'].mean())/X['Wx'].std()\n",
    "    X['Wy'] = (X['Wy'] - X['Wy'].mean())/X['Wy'].std()\n",
    "\n",
    "    return X.iloc[:,-2:]\n",
    "\n",
    "std_features = ['temperature', 'dew_point', 'pressure', 'ground_pressure',\n",
    "                'humidity',]\n",
    "minmax_features = ['clouds', 'rain', 'snow', 'ice', 'fr_rain', 'convective',\n",
    "                   'snow_depth', 'accumulated', 'hours', 'rate', 'probability']\n",
    "\n",
    "time_encoder = FunctionTransformer(time_encoding)\n",
    "wind_encoder = FunctionTransformer(wind_encoding)\n",
    "\n",
    "forecast_transformer = ColumnTransformer(\n",
    "    [\n",
    "        ('Time Encoder', time_encoder, ['prediction_utc_time']),\n",
    "        ('Wind Encoder', wind_encoder, ['wind_speed', 'wind_deg']),\n",
    "        ('Std', StandardScaler(), std_features),\n",
    "        ('MinMax', MinMaxScaler(), minmax_features),\n",
    "        # ('Drop', 'drop', ['local_time', 'utc_time', 'prediction_utc_time']),\n",
    "        ('Passthrough', 'passthrough', ['utc_time', 'prediction_utc_time']),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ").set_output(transform='pandas')\n",
    "\n",
    "\n",
    "forecast_processed = forecast_transformer.fit_transform(merged_forecast_pv)\n",
    "\n",
    "forecast_processed.columns = [column.split('__')[1] for column in forecast_processed.columns]\n",
    "display(forecast_processed)\n",
    "forecast_transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "plt.subplot(311)\n",
    "melted_forecast_df = pd.melt(forecast_processed.iloc[:, :8])\n",
    "sns.violinplot(data= melted_forecast_df.sample(frac=0.25), x='variable', y='value')\n",
    "\n",
    "plt.subplot(312)\n",
    "melted_forecast_df = pd.melt(forecast_processed.iloc[:, 8:16])\n",
    "sns.violinplot(data= melted_forecast_df.sample(frac=0.25), x='variable', y='value')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(313)\n",
    "melted_forecast_df = pd.melt(forecast_processed.iloc[:, 16:])\n",
    "sns.violinplot(data= melted_forecast_df.sample(frac=0.25), x='variable', y='value')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_processed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Sequencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Photovoltaic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is to train the model on a high number of sequences. Time series data have that in particular that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Parameters ################\n",
    "#################################\n",
    "\n",
    "INPUT_WIDTH = 48\n",
    "LABEL_WIDTH = 24\n",
    "SHIFT = 36\n",
    "\n",
    "class SequenceGenerator():\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, input_width: int, label_width: int, shift: int, number_sequences: int,\n",
    "               train_df: pd.DataFrame, val_df: pd.DataFrame =None, test_df: pd.DataFrame =None,\n",
    "               label_columns: List[str] =None):\n",
    "\n",
    "        # Store the raw data.\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "\n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in\n",
    "                                            enumerate(label_columns)}\n",
    "        self.column_indices = {name: i for i, name in\n",
    "                            enumerate(train_df.columns)}\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.number_of_sequences = number_sequences\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, self.total_window_size)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Number of sequences: {self.number_of_sequences}',\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "\n",
    "    def split_window(self, features):\n",
    "        inputs = features[self.input_slice, :]\n",
    "        labels = features[self.labels_slice, :]\n",
    "        if self.label_columns is not None:\n",
    "            labels = np.stack(\n",
    "                [labels[:, self.column_indices[name]] for name in self.label_columns],\n",
    "                axis=-1)\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "\n",
    "    def make_dataset(self, data: pd.DataFrame):\n",
    "        \"Create a dataset of x sequences of features and labels\"\n",
    "        data = np.array(data)\n",
    "        last_full_sequence_start = len(data)- self.total_window_size\n",
    "        inputs, labels = [], []\n",
    "\n",
    "        for n in tqdm(range(self.number_of_sequences)):\n",
    "            random_start = np.random.randint(0, last_full_sequence_start)\n",
    "            input, label = self.split_window(data[random_start:])\n",
    "\n",
    "            inputs.append(torch.tensor(input, dtype=torch.float32))\n",
    "            labels.append(torch.tensor(label, dtype=torch.float32))\n",
    "\n",
    "        inputs = torch.cat(inputs).view(self.number_of_sequences, self.input_width, -1)\n",
    "        labels = torch.cat(labels).view(self.number_of_sequences, self.label_width, -1)\n",
    "        return TensorDataset(inputs, labels)\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.make_dataset(self.train_df)\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        return self.make_dataset(self.val_df)\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        return self.make_dataset(self.test_df)\n",
    "\n",
    "    @property\n",
    "    def example(self):\n",
    "        \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "        result = getattr(self, '_example', None)\n",
    "        if result is None:\n",
    "            # No example batch was found, so get one from the `.train` dataset\n",
    "            result = next(iter(self.train))\n",
    "            # And cache it for next time\n",
    "            self._example = result\n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "seq_pv = SequenceGenerator(input_width= INPUT_WIDTH,\n",
    "                              label_width= LABEL_WIDTH,\n",
    "                              shift= SHIFT,\n",
    "                              number_sequences= 10_000,\n",
    "                              train_df= pv_processed,\n",
    "                              label_columns= ['electricity'])\n",
    "\n",
    "seq_pv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Forecast Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceForecastDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, number_days_forecast: int = 1,\n",
    "                 label_columns: List[str] = None):\n",
    "        self.df = df.copy().astype('float32')\n",
    "        self.forecast_hours = number_days_forecast * 24\n",
    "        self.number_of_sequences = len(self.df) // self.forecast_hours\n",
    "\n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in\n",
    "                                            enumerate(label_columns)}\n",
    "        self.column_indices = {name: i for i, name in\n",
    "                            enumerate(self.df.columns)}\n",
    "        self.feature_columns = [col for col in self.df.columns if col not in self.label_columns]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.number_of_sequences\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inputs = self.df[self.feature_columns].values\n",
    "        if self.label_columns is not None:\n",
    "            labels = self.df[[name for name in self.label_columns]].values\n",
    "        else:\n",
    "            labels = self.df['electricity'].values\n",
    "        return inputs.reshape(self.number_of_sequences, self.forecast_hours, -1)[index],\\\n",
    "            labels.reshape(self.number_of_sequences, self.forecast_hours, -1)[index]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Number of sequences: {self.number_of_sequences}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "\n",
    "    @property\n",
    "    def example(self):\n",
    "        \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "        result = getattr(self, '_example', None)\n",
    "        if result is None:\n",
    "            # No example batch was found, so get a random one from the dataset\n",
    "            random_idx = np.random.randint(0, self.number_of_sequences)\n",
    "            result = self[random_idx]\n",
    "            # And cache it for next time\n",
    "            self._example = result\n",
    "        return result\n",
    "\n",
    "\n",
    "forecast_processed = forecast_processed.select_dtypes(include=np.number)\n",
    "seq_forecast = SequenceForecastDataset(forecast_processed, label_columns=['electricity', 'probability'])\n",
    "seq_forecast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the PV data\n",
    "\n",
    "column_indices = {name: i for i, name in enumerate(pv_processed.columns)}\n",
    "\n",
    "n = len(pv_processed)\n",
    "train_df_pv = pv_processed[0:int(n*0.7)]\n",
    "val_df_pv = pv_processed[int(n*0.7):int(n*0.9)]\n",
    "test_df_pv = pv_processed[int(n*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Forecast data\n",
    "# Careful: split must be done for 24h data\n",
    "\n",
    "column_indices = {name: i for i, name in enumerate(forecast_processed.columns)}\n",
    "\n",
    "n = len(forecast_processed)\n",
    "train_df_forecast = forecast_processed[0:int(n*0.7 / 24) * 24]\n",
    "val_df_forecast = forecast_processed[int(n*0.7 / 24) * 24:int(n*0.9 / 24) * 24]\n",
    "test_df_forecast = forecast_processed[int(n*0.9 / 24) * 24:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Baseline metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 Compute regression metrics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_regression_metrics(model, dataloader):\n",
    "    y_preds = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            y_preds.append(outputs.cpu())\n",
    "            labels.append(targets.cpu())\n",
    "\n",
    "    y_preds = torch.cat(y_preds)\n",
    "    labels = torch.cat(labels)\n",
    "\n",
    "    # Compute metrics\n",
    "    mse = torch.mean((y_preds - labels) ** 2)\n",
    "    rmse = mse ** 0.5\n",
    "    mae = torch.mean(abs(y_preds - labels))\n",
    "    r2 = 1 - torch.sum((y_preds - labels) ** 2) / torch.sum(y_preds - torch.mean(labels) ** 2)\n",
    "\n",
    "    return {\"mse\": mse, \"rmse\": rmse, \"mae\": mae, \"r2\": r2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 Photovoltaic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_pv = SequenceGenerator(input_width= INPUT_WIDTH,\n",
    "                              label_width= LABEL_WIDTH,\n",
    "                              shift= SHIFT,\n",
    "                              number_sequences= 10_000,\n",
    "                              train_df= train_df_pv,\n",
    "                              val_df= val_df_pv,\n",
    "                              test_df= test_df_pv,\n",
    "                              label_columns= ['electricity'])\n",
    "\n",
    "train_dataset_pv = sequences_pv.train\n",
    "val_dataset_pv = sequences_pv.val\n",
    "test_dataset_pv = sequences_pv.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader_pv = DataLoader(test_dataset_pv, batch_size= 32, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselinePV(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[:, 12:-12, 4]\n",
    "        return x.view(len(x), -1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_pv = compute_regression_metrics(BaselinePV(), test_loader_pv)\n",
    "\n",
    "print(\"Baseline Metrics:\", baseline_pv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 Forecast data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stick day_ahead and day_before electricity data to the forecast data\n",
    "pv_data['local_time'] = pd.to_datetime(pv_data['local_time'], utc= True)\n",
    "merged_forecast_pv = pd.merge(pv_data[['local_time', 'electricity']], forecast_clean, left_on='local_time', right_on= 'prediction_utc_time', how='inner')\n",
    "merged_forecast_pv = merged_forecast_pv.rename(columns= {'electricity': 'electricity_day_ahead'})\n",
    "\n",
    "merged_forecast_pv['baseline_day_utc_time'] = merged_forecast_pv['prediction_utc_time'] - dt.timedelta(days= 2)\n",
    "merged_forecast_pv = pd.merge(pv_data[['local_time', 'electricity']], merged_forecast_pv, left_on='local_time', right_on= 'baseline_day_utc_time', how='inner')\n",
    "merged_forecast_pv = merged_forecast_pv.rename(columns= {'electricity': 'electricity_day_before'})\n",
    "\n",
    "time_cols_mask = [col for col in merged_forecast_pv.columns if 'time' not in col]\n",
    "\n",
    "baseline_forecast_df = merged_forecast_pv[time_cols_mask]\n",
    "baseline_forecast_dataset = SequenceForecastDataset(df= baseline_forecast_df,\n",
    "                                                label_columns= ['electricity_day_ahead'])\n",
    "display(baseline_forecast_dataset.df), baseline_forecast_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Forecast data\n",
    "# Careful: split must be done for 24h data\n",
    "\n",
    "column_indices = {name: i for i, name in enumerate(baseline_forecast_df.columns)}\n",
    "\n",
    "n = len(baseline_forecast_df)\n",
    "train_baseline_df_forecast = baseline_forecast_df[0:int(n*0.7 / 24) * 24]\n",
    "val_baseline_df_forecast = baseline_forecast_df[int(n*0.7 / 24) * 24:int(n*0.9 / 24) * 24]\n",
    "test_df_baseline_forecast = baseline_forecast_df[int(n*0.9 / 24) * 24:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_baseline_dataset = SequenceForecastDataset(df= test_df_baseline_forecast,\n",
    "                                            label_columns= ['electricity_day_ahead'])\n",
    "\n",
    "test_baseline_loader_forecast = DataLoader(test_baseline_dataset, batch_size= 32, shuffle= True)\n",
    "test_baseline_dataset.column_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineForecast(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[:, :, 0]\n",
    "        return x.view(len(x), -1, 1)\n",
    "\n",
    "baseline_forecast = compute_regression_metrics(BaselineForecast(), test_baseline_loader_forecast)\n",
    "print(\"Baseline Metrics:\", baseline_forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Prepare the datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the PV data\n",
    "\n",
    "column_indices = {name: i for i, name in enumerate(pv_processed.columns)}\n",
    "\n",
    "n = len(pv_processed)\n",
    "train_df_pv = pv_processed[0:int(n*0.7)]\n",
    "val_df_pv = pv_processed[int(n*0.7):int(n*0.9)]\n",
    "test_df_pv = pv_processed[int(n*0.9):]\n",
    "\n",
    "sequences_pv = SequenceGenerator(input_width= INPUT_WIDTH,\n",
    "                              label_width= LABEL_WIDTH,\n",
    "                              shift= SHIFT,\n",
    "                              number_sequences= 10_000,\n",
    "                              train_df= train_df_pv,\n",
    "                              val_df= val_df_pv,\n",
    "                              test_df= test_df_pv,\n",
    "                              label_columns= ['electricity'])\n",
    "\n",
    "train_dataset_pv = sequences_pv.train\n",
    "val_dataset_pv = sequences_pv.val\n",
    "test_dataset_pv = sequences_pv.test\n",
    "\n",
    "train_loader_pv = DataLoader(train_dataset_pv, batch_size= 32, shuffle= True)\n",
    "val_loader_pv = DataLoader(val_dataset_pv, batch_size= 32, shuffle= True)\n",
    "test_loader_pv = DataLoader(test_dataset_pv, batch_size= 32, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_forecast = SequenceForecastDataset(df= train_df_forecast, label_columns=['electricity'])\n",
    "val_dataset_forecast = SequenceForecastDataset(df= val_df_forecast, label_columns=['electricity'])\n",
    "test_dataset_forecast = SequenceForecastDataset(df= test_df_forecast, label_columns=['electricity'])\n",
    "\n",
    "train_loader_forecast = DataLoader(train_dataset_forecast, batch_size= 16, shuffle= True)\n",
    "val_loader_forecast = DataLoader(val_dataset_forecast, batch_size= 16, shuffle= True)\n",
    "test_loader_forecast = DataLoader(test_dataset_forecast, batch_size= 16, shuffle= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Simple LSTM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, p: int):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.n_features = p\n",
    "        self.lstm = nn.LSTM(p, 24, batch_first= True)\n",
    "        self.linear = nn.Linear(24,24)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = F.tanh(x)\n",
    "        x = self.linear(x[:,-1,:])\n",
    "        return x.view(len(x), -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, p: int):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.n_features = p\n",
    "        self.lstm = nn.RNN(p, 24, batch_first= True)\n",
    "        self.linear = nn.Linear(24,24)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = F.tanh(x)\n",
    "        x = self.linear(x[:,-1,:])\n",
    "        return x.view(len(x), -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_forecast.__len__(), val_loader_forecast.__len__(), test_loader_forecast.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Data ##############\n",
    "dataset = 'forecast'  # ['pv', 'forecast']\n",
    "\n",
    "########## Parameters #########\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "epochs = 3\n",
    "\n",
    "\n",
    "if dataset == 'pv':\n",
    "    train_dataset = train_dataset_pv\n",
    "    val_dataset = val_dataset_pv\n",
    "    test_dataset = test_dataset_pv\n",
    "    n_features = train_dataset_pv.tensors[0].shape[-1]\n",
    "else:\n",
    "    train_dataset = train_dataset_forecast\n",
    "    val_dataset = val_dataset_forecast\n",
    "    test_dataset = test_dataset_forecast\n",
    "    n_features = train_dataset.example[0].shape[-1]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size= batch_size, shuffle= True)\n",
    "val_loader = DataLoader(val_dataset, batch_size= batch_size, shuffle= True)\n",
    "test_loader = DataLoader(test_dataset, batch_size= batch_size, shuffle= True)\n",
    "\n",
    "model = RNNModel(p = n_features)\n",
    "loss_fn = nn.MSELoss()\n",
    "optim = torch.optim.Adam(params= model.parameters(), lr= learning_rate)\n",
    "\n",
    "\n",
    "def training_one_epoch(model, train_dataloader, val_dataloader):\n",
    "    size = len(train_dataloader.dataset)\n",
    "    running_loss = 0\n",
    "    earlystopping = 0\n",
    "    for batch, data in enumerate(train_dataloader):\n",
    "        X, y = data\n",
    "        output = model(X)\n",
    "        loss = loss_fn(output, y)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if batch % 10 == 9:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"\\tloss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = []\n",
    "        labels = []\n",
    "        vsize = len(val_dataloader.dataset)\n",
    "        running_vloss = 0.0\n",
    "\n",
    "        # In evaluation mode some model specific operations can be omitted eg. dropout layer\n",
    "        model.train(False) # Switching to evaluation mode, eg. turning off regularisation\n",
    "        for j, vdata in enumerate(val_dataloader):\n",
    "            vinputs, vlabels = vdata\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss.item()\n",
    "\n",
    "            if j % 10 == 9:\n",
    "                vloss, vcurrent = vloss.item(), j * batch_size + len(vinputs)\n",
    "                print(f\"\\tval loss: {vloss:>7f}  [{vcurrent:>5d}/{vsize:>5d}]\")\n",
    "\n",
    "        model.train(True) # Switching back to training mode, eg. turning on regularisation\n",
    "\n",
    "        # avg_loss = running_loss / 1000\n",
    "        # avg_vloss = running_vloss / len(val_loader_forecast)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch + 1}:')\n",
    "    model.train()\n",
    "    training_one_epoch(model, train_loader, val_loader)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "metrics = compute_regression_metrics(model, test_loader)\n",
    "print(\"Regression Metrics:\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you start TensorBoard at the command line and open it in a new browser tab (usually at [localhost:6006](localhost:6006)), you should see the image grid under the IMAGES tab.\n",
    "\n",
    "## Graphing Scalars to Visualize Training\n",
    "\n",
    "TensorBoard is useful for tracking the progress and efficacy of your training. Below, we'll run a training loop, track some metrics, and save the data for TensorBoard's consumption.\n",
    "\n",
    "Let's define a model to categorize our image tiles, and an optimizer and loss function for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Default log_dir argument is \"runs\" - but it's good to be specific\n",
    "# torch.utils.tensorboard.SummaryWriter is imported above\n",
    "writer = SummaryWriter()\n",
    "# writer.close()\n",
    "\n",
    "# Write image data to TensorBoard log dir\n",
    "# writer.add_image('Four Fashion-MNIST Images', img_grid)\n",
    "# writer.flush()\n",
    "\n",
    "# To view, start TensorBoard on the command line with:\n",
    "#   tensorboard --logdir=runs\n",
    "# ...and open a browser tab to http://localhost:6006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Data ##############\n",
    "dataset = 'forecast'  # ['pv', 'forecast']\n",
    "\n",
    "########## Parameters #########\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "if dataset == 'pv':\n",
    "    train_dataset = train_dataset_pv\n",
    "    test_dataset = test_dataset_pv\n",
    "    n_features = train_dataset_pv.tensors[0].shape[-1]\n",
    "else:\n",
    "    train_dataset = train_dataset_forecast\n",
    "    test_dataset = test_dataset_forecast\n",
    "    n_features = train_dataset.example[0].shape[-1]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size= batch_size, shuffle= True)\n",
    "test_loader = DataLoader(test_dataset, batch_size= batch_size, shuffle= True)\n",
    "\n",
    "model = LSTMModel(p = n_features)\n",
    "loss_fn = nn.MSELoss()\n",
    "optim = torch.optim.Adam(params= model.parameters(), lr= learning_rate)\n",
    "\n",
    "print(len(val_loader_forecast))\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(train_loader_forecast, 0):\n",
    "        # basic training loop\n",
    "        inputs, labels = data\n",
    "        optim.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 8 == 7:    # Every 1000 mini-batches...\n",
    "            print('Batch {}'.format(i + 1))\n",
    "            # Check against the validation set\n",
    "            running_vloss = 0.0\n",
    "\n",
    "            # In evaluation mode some model specific operations can be omitted eg. dropout layer\n",
    "            model.train(False) # Switching to evaluation mode, eg. turning off regularisation\n",
    "            for j, vdata in enumerate(val_loader_forecast, 0):\n",
    "                vinputs, vlabels = vdata\n",
    "                voutputs = model(vinputs)\n",
    "                vloss = loss_fn(voutputs, vlabels)\n",
    "                running_vloss += vloss.item()\n",
    "            model.train(True) # Switching back to training mode, eg. turning on regularisation\n",
    "\n",
    "            avg_loss = running_loss / 1000\n",
    "            avg_vloss = running_vloss / len(val_loader_forecast)\n",
    "\n",
    "            # Log the running loss averaged per batch\n",
    "            writer.add_scalars('Training vs. Validation Loss',\n",
    "                            { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                            epoch * len(train_loader_forecast) + i)\n",
    "\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')\n",
    "\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switch to your open TensorBoard and have a look at the SCALARS tab.\n",
    "\n",
    "## Visualizing Your Model\n",
    "TensorBoard can also be used to examine the data flow within your model. To do this, call the `add_graph()` method with a model and sample input. When you open "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, grab a single mini-batch of images\n",
    "dataiter = iter(train_loader_forecast)\n",
    "inputs, labels = next(dataiter)\n",
    "\n",
    "# add_graph() will trace the sample input through your model,\n",
    "# and render it as a graph.\n",
    "writer.add_graph(model, inputs)\n",
    "writer.flush()\n",
    "writer.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_pv, baseline_forecast"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
